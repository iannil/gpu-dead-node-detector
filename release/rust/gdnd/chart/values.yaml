# GDND Helm Chart Values
# GPU Dead Node Detector configuration

# Image configuration
# IMPORTANT: Update repository to include your registry before deployment
# Examples:
#   - Docker Hub: docker.io/myorg/gdnd
#   - Private registry: registry.example.com/gdnd
#   - Cloud registry: gcr.io/my-project/gdnd, xxx.dkr.ecr.region.amazonaws.com/gdnd
image:
  # Container image repository (must include registry for production)
  repository: "your-registry.example.com/gdnd"
  # Image tag (use specific version tags in production, avoid "latest")
  tag: "1.0.0"
  pullPolicy: IfNotPresent

# Image pull secrets (required for private registries)
# Example:
#   imagePullSecrets:
#     - name: my-registry-secret
imagePullSecrets: []

# Resource limits
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 50m
    memory: 64Mi

# Node selector - by default selects nodes with NVIDIA GPUs
nodeSelector:
  nvidia.com/gpu: "true"

# Tolerations for GPU nodes
tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Pod priority class
priorityClassName: system-node-critical

# Service account configuration
serviceAccount:
  create: true
  name: ""
  annotations: {}

# RBAC configuration
rbac:
  create: true

# GDND configuration
config:
  # Device type: auto, nvidia, ascend
  deviceType: auto

  # L1 passive detection interval
  l1Interval: "30s"

  # L2 active detection interval
  l2Interval: "5m"

  # L3 PCIe detection interval
  l3Interval: "24h"
  l3Enabled: false

  # Path to gpu-check binary
  gpuCheckPath: "/usr/local/bin/gpu-check"

  # Health check configuration
  health:
    # Number of consecutive failures before marking as UNHEALTHY
    failureThreshold: 3
    # Fatal XID error codes that trigger immediate isolation
    fatalXids:
      - 31  # GPU memory page fault
      - 43  # GPU stopped processing
      - 48  # Double Bit ECC Error
      - 79  # GPU has fallen off the bus
    # Temperature threshold in Celsius
    temperatureThreshold: 85
    # Timeout for active check operations
    activeCheckTimeout: "5s"

  # Isolation action configuration
  isolation:
    # Whether to cordon the node
    cordon: true
    # Whether to evict existing pods
    evictPods: false
    # Taint configuration
    taintKey: "nvidia.com/gpu-health"
    taintValue: "failed"
    taintEffect: "NoSchedule"

  # Dry run mode - log actions but don't execute
  dryRun: false

  # Self-healing configuration
  # WARNING: Healing operations may interrupt running GPU workloads
  # Use with caution in production environments
  healing:
    # Enable self-healing (disabled by default for safety)
    enabled: false
    # Healing strategy:
    #   - conservative: Only kill zombie processes (safest)
    #   - moderate: Kill zombies + GPU soft reset
    #   - aggressive: All recovery options including driver reload (DANGEROUS)
    strategy: conservative
    # Run healing operations in dry-run mode (log but don't execute)
    dryRun: false
    # Timeout for healing operations
    timeout: "30s"

  # Recovery detection configuration
  # Allow isolated GPUs to return to healthy state after recovery
  recovery:
    # Enable recovery detection (disabled by default)
    enabled: false
    # Number of consecutive healthy checks before recovery
    threshold: 5
    # Interval between recovery checks
    interval: "5m"

# Prometheus metrics configuration
metrics:
  enabled: true
  port: 9100
  path: "/metrics"

  # ServiceMonitor for Prometheus Operator
  serviceMonitor:
    enabled: false
    namespace: ""
    interval: "30s"
    labels: {}

# Logging configuration
logging:
  level: info
  format: json

# Pod annotations
podAnnotations: {}

# Pod labels
podLabels: {}

# Security context
# GDND requires privileged access for:
#   - GPU device access (/dev/nvidia*)
#   - NVML library access
#   - Reading /proc for zombie process detection
# This requires Pod Security Standard: privileged
securityContext:
  privileged: true
  # Recommended: drop all capabilities and add only what's needed
  # However, GPU access typically requires privileged mode
  capabilities:
    add:
      - SYS_ADMIN  # Required for NVML operations
    drop:
      - ALL

# Pod security context
# Note: GDND requires privileged namespace for GPU access
# Ensure namespace has: pod-security.kubernetes.io/enforce: privileged
podSecurityContext:
  # Run as root for GPU device access
  runAsUser: 0
  runAsGroup: 0
  fsGroup: 0

# Extra environment variables
extraEnv: []

# Extra volume mounts
extraVolumeMounts: []

# Extra volumes
extraVolumes: []

# Network Policy configuration
# Enable to restrict pod network access in security-sensitive environments
networkPolicy:
  # Set to true to create NetworkPolicy
  enabled: false

  # Kubernetes API server access configuration
  apiServer:
    # API server port (default: 6443)
    port: 6443
    # API server CIDR (leave empty to allow all, recommended to set for production)
    # Example: "10.0.0.1/32" for specific API server IP
    cidr: ""

  # Prometheus scraping configuration
  prometheus:
    # Namespace selector for Prometheus pods
    # Example:
    #   namespaceSelector:
    #     matchLabels:
    #       name: monitoring
    namespaceSelector: {}
    # Pod selector for Prometheus pods
    # Example:
    #   podSelector:
    #     matchLabels:
    #       app: prometheus
    podSelector: {}

